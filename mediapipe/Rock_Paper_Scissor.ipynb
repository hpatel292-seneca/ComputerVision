{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc96cd4-4ef9-4baa-b2bc-dc96d328bbfc",
   "metadata": {},
   "source": [
    "Here's a detailed explanation of each part of the code:\n",
    "\n",
    "### 1. **Importing Required Libraries:**\n",
    "```python\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "```\n",
    "- **OpenCV (cv2)**: Used for capturing video frames from the webcam and displaying the output.\n",
    "- **MediaPipe (mp)**: Google's framework for multimodal perception, used here to detect hand landmarks.\n",
    "\n",
    "### 2. **Initializing MediaPipe Hands and Drawing Utilities:**\n",
    "```python\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "```\n",
    "- **mp_hands**: We initialize MediaPipe's hand detection solution.\n",
    "- **mp_drawing**: MediaPipe provides utilities to draw landmarks on images.\n",
    "\n",
    "### 3. **Gesture Classification Function:**\n",
    "```python\n",
    "def classify_hand_gesture(hand_landmarks):\n",
    "    # We will use the tip of the fingers (landmarks: 8, 12, 16, 20) and the thumb (landmark: 4)\n",
    "    # For simplicity, let's define:\n",
    "    # Rock: All fingers are closed (folded)\n",
    "    # Paper: All fingers are open\n",
    "    # Scissors: Index and middle finger are open, others are closed\n",
    "```\n",
    "- This function classifies a hand gesture based on the detected hand landmarks (specific points on the hand).\n",
    "- Each finger's tip has a specific landmark number. For example:\n",
    "  - **Thumb**: 4\n",
    "  - **Index**: 8\n",
    "  - **Middle**: 12\n",
    "  - **Ring**: 16\n",
    "  - **Pinky**: 20\n",
    "- **Rock**: All fingers are closed (i.e., the tips are below the base joints).\n",
    "- **Paper**: All fingers are open (i.e., the tips are above the base joints).\n",
    "- **Scissors**: Only the index and middle fingers are open, while the rest are closed.\n",
    "\n",
    "```python\n",
    "    thumb_tip = hand_landmarks.landmark[4].y\n",
    "    index_tip = hand_landmarks.landmark[8].y\n",
    "    middle_tip = hand_landmarks.landmark[12].y\n",
    "    ring_tip = hand_landmarks.landmark[16].y\n",
    "    pinky_tip = hand_landmarks.landmark[20].y\n",
    "```\n",
    "- This part retrieves the **Y-coordinate** of the fingertip positions (landmarks) to determine whether a finger is open or closed.\n",
    "  - A lower Y value (closer to the top of the screen) indicates that the finger is open.\n",
    "  \n",
    "```python\n",
    "    index_base = hand_landmarks.landmark[5].y\n",
    "    middle_base = hand_landmarks.landmark[9].y\n",
    "    ring_base = hand_landmarks.landmark[13].y\n",
    "    pinky_base = hand_landmarks.landmark[17].y\n",
    "```\n",
    "- This retrieves the Y-coordinates of the base joints (second joints from the tips) of each finger to compare with the fingertip positions.\n",
    "\n",
    "```python\n",
    "    # Determine if fingers are open (tip is above the base) or closed\n",
    "    thumb_open = hand_landmarks.landmark[4].x < hand_landmarks.landmark[3].x  # Thumb is open if x position of tip is less than the joint\n",
    "    index_open = index_tip < index_base\n",
    "    middle_open = middle_tip < middle_base\n",
    "    ring_open = ring_tip < ring_base\n",
    "    pinky_open = pinky_tip < pinky_base\n",
    "```\n",
    "- **thumb_open**: The thumb's openness is determined by checking the **X-coordinate** because the thumb moves sideways (along the X-axis), unlike other fingers.\n",
    "- For other fingers, openness is determined by comparing the Y-coordinates of the tip and base joints. If the tip is higher (smaller Y value), the finger is considered open.\n",
    "\n",
    "```python\n",
    "    # Classify gesture\n",
    "    if not thumb_open and not index_open and not middle_open and not ring_open and not pinky_open:\n",
    "        return \"Rock\"\n",
    "    elif index_open and middle_open and not ring_open and not pinky_open:\n",
    "        return \"Scissors\"\n",
    "    elif index_open and middle_open and ring_open and pinky_open:\n",
    "        return \"Paper\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "```\n",
    "- This section classifies the gesture based on the state of the fingers:\n",
    "  - **Rock**: All fingers are closed.\n",
    "  - **Scissors**: Index and middle fingers are open, but ring and pinky fingers are closed.\n",
    "  - **Paper**: All fingers are open.\n",
    "  - If the finger configuration doesn't match any of these patterns, it returns \"Unknown.\"\n",
    "\n",
    "### 4. **Webcam Capture and Hand Detection:**\n",
    "```python\n",
    "cap = cv2.VideoCapture(0)\n",
    "```\n",
    "- Opens the webcam feed (`0` indicates the default camera).\n",
    "\n",
    "### 5. **MediaPipe Hand Detection:**\n",
    "```python\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5) as hands:\n",
    "```\n",
    "- This initializes the **MediaPipe Hands** model with a detection confidence of 0.7 and a tracking confidence of 0.5.\n",
    "  - **min_detection_confidence**: Minimum confidence value to detect hands.\n",
    "  - **min_tracking_confidence**: Minimum confidence value to track hand landmarks over time.\n",
    "\n",
    "### 6. **Main Loop to Process Video Frames:**\n",
    "```python\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "```\n",
    "- A loop is started to continuously read frames from the webcam.\n",
    "\n",
    "```python\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "```\n",
    "- The frame is flipped horizontally for a mirror effect.\n",
    "- The frame is converted from BGR (OpenCV's default) to RGB because MediaPipe requires RGB images.\n",
    "- `hands.process()` detects hands in the frame and returns the landmarks.\n",
    "\n",
    "### 7. **Processing Detected Hands:**\n",
    "```python\n",
    "if result.multi_hand_landmarks:\n",
    "    for hand_landmarks in result.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "```\n",
    "- If hands are detected, the landmarks are drawn on the frame using `mp_drawing.draw_landmarks()`.\n",
    "\n",
    "```python\n",
    "        gesture = classify_hand_gesture(hand_landmarks)\n",
    "        cv2.putText(frame, f'Gesture: {gesture}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "```\n",
    "- The detected gesture is classified using the `classify_hand_gesture()` function and displayed on the frame.\n",
    "\n",
    "### 8. **Displaying the Result and Quitting:**\n",
    "```python\n",
    "cv2.imshow('Rock, Paper, Scissors Gesture Detection', frame)\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    break\n",
    "```\n",
    "- The frame is displayed with the detected gesture, and the loop continues until the user presses the \"q\" key to quit.\n",
    "\n",
    "### 9. **Releasing Resources:**\n",
    "```python\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "- After the loop is exited, the webcam is released, and the OpenCV windows are closed.\n",
    "\n",
    "### Summary:\n",
    "- The code captures video from a webcam and uses MediaPipe to detect hand landmarks.\n",
    "- Based on the finger positions, the code classifies the hand gesture as either \"Rock,\" \"Paper,\" or \"Scissors.\"\n",
    "- The gesture is displayed on the video feed in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "361735f1-66d7-4795-bcab-618f5ccff8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe hands and drawing utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Function to classify gestures\n",
    "def classify_hand_gesture(hand_landmarks):\n",
    "    # We will use the tip of the fingers (landmarks: 8, 12, 16, 20) and the thumb (landmark: 4)\n",
    "    # For simplicity, let's define:\n",
    "    # Rock: All fingers are closed (folded)\n",
    "    # Paper: All fingers are open\n",
    "    # Scissors: Index and middle finger are open, others are closed\n",
    "\n",
    "    # Get landmarks\n",
    "    thumb_tip = hand_landmarks.landmark[4].y\n",
    "    index_tip = hand_landmarks.landmark[8].y\n",
    "    middle_tip = hand_landmarks.landmark[12].y\n",
    "    ring_tip = hand_landmarks.landmark[16].y\n",
    "    pinky_tip = hand_landmarks.landmark[20].y\n",
    "\n",
    "    # Get the base of each finger (second joint from tip)\n",
    "    index_base = hand_landmarks.landmark[5].y\n",
    "    middle_base = hand_landmarks.landmark[9].y\n",
    "    ring_base = hand_landmarks.landmark[13].y\n",
    "    pinky_base = hand_landmarks.landmark[17].y\n",
    "\n",
    "    # Determine if fingers are open (tip is above the base) or closed\n",
    "    thumb_open = hand_landmarks.landmark[4].x < hand_landmarks.landmark[6].x\n",
    "    index_open = index_tip < index_base\n",
    "    middle_open = middle_tip < middle_base\n",
    "    ring_open = ring_tip < ring_base\n",
    "    pinky_open = pinky_tip < pinky_base\n",
    "\n",
    "    # Classify gesture\n",
    "    if not thumb_open and not index_open and not middle_open and not ring_open and not pinky_open:\n",
    "        return \"Rock\"\n",
    "    elif index_open and middle_open and not ring_open and not pinky_open:\n",
    "        return \"Scissors\"\n",
    "    elif index_open and middle_open and ring_open and pinky_open:\n",
    "        return \"Paper\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Create MediaPipe hands detector\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Flip the frame horizontally for a natural selfie-view display\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame to detect hands\n",
    "        result = hands.process(rgb_frame)\n",
    "\n",
    "        # If hand landmarks are detected\n",
    "        if result.multi_hand_landmarks:\n",
    "            for hand_landmarks in result.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the frame\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Classify the gesture\n",
    "                gesture = classify_hand_gesture(hand_landmarks)\n",
    "\n",
    "                # Display the classified gesture on the frame\n",
    "                cv2.putText(frame, f'Gesture: {gesture}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow('Rock, Paper, Scissors Gesture Detection', frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6cf34-2b2e-4598-ae85-cfe4bf166e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
